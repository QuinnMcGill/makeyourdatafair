{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massaging to Improve Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Creating the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\aif360\\algorithms\\preprocessing\\optim_preproc_helpers\\data_preproc_functions.py:50: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sex'] = df['sex'].replace({'Female': 0.0, 'Male': 1.0})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, classification_report, recall_score, f1_score\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult\n",
    "\n",
    "# Import files for Fairness Metrics\n",
    "from individual_fairness import eval_ind_fairness \n",
    "from disparate_impact import eval_disparate_impact\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "dataset_orig = load_preproc_data_adult(['sex'])\n",
    "np.random.seed(42)\n",
    "\n",
    "# Convert the Dataset into a Dataframe for easier massaging\n",
    "df_original = pd.DataFrame(columns=dataset_orig.feature_names, data=dataset_orig.features)\n",
    "df_original['Income Binary'] = dataset_orig.labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_performance(y_test, y_pred):\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Display metrics\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masaging to Remove Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discrimination(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Compute the discrimination score (difference in positive outcome rates).\n",
    "    privileged = df[df[sensitive_attr] == privileged_value]\n",
    "    unprivileged = df[df[sensitive_attr] != privileged_value]\n",
    "\n",
    "    pos_rate_privileged = sum(privileged[class_attr] == positive_class) / len(privileged)\n",
    "    pos_rate_unprivileged = sum(unprivileged[class_attr] == positive_class) / len(unprivileged)\n",
    "\n",
    "    return pos_rate_privileged - pos_rate_unprivileged\n",
    "\n",
    "def compute_m(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Compute number of instances M to relabel.\n",
    "    disc = compute_discrimination(df, sensitive_attr, class_attr, privileged_value, positive_class)\n",
    "    \n",
    "    n_privileged = len(df[df[sensitive_attr] == privileged_value])\n",
    "    n_unprivileged = len(df[df[sensitive_attr] != privileged_value])\n",
    "    \n",
    "    return int(disc * (n_privileged * n_unprivileged) / len(df))\n",
    "\n",
    "def rank_instances(df, features, sensitive_attr, class_attr):\n",
    "    # Train a classifier to rank instances by likelihood of being positive.\n",
    "    X = df[features]\n",
    "    y = df[class_attr]\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    scores = model.predict_proba(X)[:, 1]  # Probability of positive class\n",
    "    df['score'] = scores\n",
    "    return df\n",
    "\n",
    "def apply_massaging(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Perform massaging technique.\n",
    "    # Step 1: Compute M\n",
    "    M = compute_m(df, sensitive_attr, class_attr, privileged_value, positive_class)\n",
    "    print(f\"Number of label changes (M): {M}\")\n",
    "\n",
    "    if M == 0:\n",
    "        print(\"No massaging needed.\")\n",
    "        return df\n",
    "\n",
    "    # Step 2: Rank instances\n",
    "    features = [col for col in df.columns if col not in [sensitive_attr, class_attr]]\n",
    "    df = rank_instances(df, features, sensitive_attr, class_attr)\n",
    "\n",
    "    # Step 3: Modify labels\n",
    "    unprivileged_neg = df[(df[sensitive_attr] != privileged_value) & (df[class_attr] != positive_class)]\n",
    "    privileged_pos = df[(df[sensitive_attr] == privileged_value) & (df[class_attr] == positive_class)]\n",
    "\n",
    "    # Promote top M from unprivileged_neg\n",
    "    df.loc[unprivileged_neg.nlargest(M, 'score').index, class_attr] = positive_class\n",
    "\n",
    "    # Demote bottom M from privileged_pos\n",
    "    df.loc[privileged_pos.nsmallest(M, 'score').index, class_attr] = 1 - positive_class\n",
    "\n",
    "    # Drop the ranking column\n",
    "    df.drop(columns=['score'], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8043\n",
      "Precision: 0.6587\n",
      "Recall: 0.3781\n",
      "F1 Score: 0.4804\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.88      7431\n",
      "         1.0       0.66      0.38      0.48      2338\n",
      "\n",
      "    accuracy                           0.80      9769\n",
      "   macro avg       0.74      0.66      0.68      9769\n",
      "weighted avg       0.79      0.80      0.78      9769\n",
      "\n",
      "(3259, 2)\n",
      "(6510, 2)\n",
      "Underprivileged Rate: 0\n",
      "Privileged Rate: 0.20614439324116743\n",
      "Disparate Impact Score for Baseline: 0.0000\n"
     ]
    }
   ],
   "source": [
    "scale_orig = StandardScaler()\n",
    "lmod = LogisticRegression()\n",
    "\n",
    "x_orig = df_original.drop(columns=['Income Binary'])\n",
    "y_orig = df_original['Income Binary']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "\n",
    "# Scale numerical features for training\n",
    "x_train = scale_orig.fit_transform(x_train)\n",
    "\n",
    "# Train the model\n",
    "lmod.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "x_test_scaled = scale_orig.fit_transform(x_test)\n",
    "y_pred = lmod.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate performance of baseline model\n",
    "eval_performance(y_test, y_pred)\n",
    "\n",
    "# Evaluate Individual Fairness\n",
    "ind_fairness_orig = eval_ind_fairness(x_train, y_train, x_test_scaled, y_pred)\n",
    "print(f'Individual Fairness For Baseline: {ind_fairness_orig:.4f}')\n",
    "\n",
    "# Evaluate Disparate Impact\n",
    "disparate_impact_orig = eval_disparate_impact(x_test['sex'], y_pred)\n",
    "print(f'Disparate Impact Score for Baseline: {disparate_impact_orig:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance After Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label changes (M): 2105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8634\n",
      "Precision: 0.7290\n",
      "Recall: 0.6835\n",
      "F1 Score: 0.7055\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91      7431\n",
      "         1.0       0.73      0.68      0.71      2338\n",
      "\n",
      "    accuracy                           0.86      9769\n",
      "   macro avg       0.82      0.80      0.81      9769\n",
      "weighted avg       0.86      0.86      0.86      9769\n",
      "\n",
      "(3288, 2)\n",
      "(6481, 2)\n",
      "Underprivileged Rate: 646\n",
      "Privileged Rate: 0.2385434346551458\n",
      "Disparate Impact Score After Massaging: 0.8236\n"
     ]
    }
   ],
   "source": [
    "# Apply massaging technique\n",
    "df_massaged = apply_massaging(df_original, 'sex', 'Income Binary', privileged_value=1, positive_class=1)\n",
    "\n",
    "# Repeat training and testing with massaged data\n",
    "x_massaged = df_massaged.drop(columns=['Income Binary'])\n",
    "y_massaged = df_massaged['Income Binary']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_massaged, y_massaged, test_size=0.2, random_state=42, stratify=y_massaged)\n",
    "\n",
    "# Scale numerical features for training\n",
    "x_train = scale_orig.fit_transform(x_train)\n",
    "\n",
    "# Train the model\n",
    "lmod.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "x_test_scaled = scale_orig.fit_transform(x_test)\n",
    "y_pred = lmod.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate performance of baseline model\n",
    "eval_performance(y_test, y_pred)\n",
    "\n",
    "# Evaluate Individual Fairness\n",
    "ind_fairness_massaged = eval_ind_fairness(x_train, y_train, x_test_scaled, y_pred)\n",
    "print(f'Individual Fairness Ater Massaging: {ind_fairness_massaged:.4f}')\n",
    "\n",
    "# Evaluate Disparate Impact\n",
    "disparate_impact_massaged = eval_disparate_impact(x_test['sex'], y_pred) # x_test[:,1] is the column for the 'sex' attribute\n",
    "print(f'Disparate Impact Score After Massaging: {disparate_impact_massaged:.4f}')"
    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "This version of the massaging technique uses the same preprocessing as adult_reweighing.ipynb (from AIF360) for the baseline model. The baseline model's performance is nearly identical to that found in adult_reweighing.ipynb. \n",
    "\n",
    "## Results\n",
    "\n",
    "Massaging the data improved the model's predictive performance. This could be because the baseline model relies to heavily on the \"sex\" attribute. The model's individual fairness increased significantly after massaging. The disparate impact of the model increased with massaging. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
