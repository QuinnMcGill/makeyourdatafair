{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massaging to Improve Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Creating the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\aif360\\algorithms\\preprocessing\\optim_preproc_helpers\\data_preproc_functions.py:50: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sex'] = df['sex'].replace({'Female': 0.0, 'Male': 1.0})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, classification_report, recall_score, f1_score\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult\n",
    "\n",
    "# Import files for Fairness Metrics\n",
    "from individual_fairness import eval_ind_fairness \n",
    "from disparate_impact import eval_disparate_impact\n",
    "from counterfactual_fairness import evaluate_counterfactual_fairness_sex\n",
    "from equality_fairness import eval_equality\n",
    "from group_fairness import eval_group_fairness\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "dataset_orig = load_preproc_data_adult(['sex'])\n",
    "np.random.seed(42)\n",
    "\n",
    "# Convert the Dataset into a Dataframe for easier massaging\n",
    "df_original = pd.DataFrame(columns=dataset_orig.feature_names, data=dataset_orig.features)\n",
    "df_original['Income Binary'] = dataset_orig.labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_performance(y_test, y_pred):\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Display metrics\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(\"\\nClassification Report:\\n\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masaging to Remove Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discrimination(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Compute the discrimination score (difference in positive outcome rates).\n",
    "    privileged = df[df[sensitive_attr] == privileged_value]\n",
    "    unprivileged = df[df[sensitive_attr] != privileged_value]\n",
    "\n",
    "    pos_rate_privileged = sum(privileged[class_attr] == positive_class) / len(privileged)\n",
    "    pos_rate_unprivileged = sum(unprivileged[class_attr] == positive_class) / len(unprivileged)\n",
    "\n",
    "    return pos_rate_privileged - pos_rate_unprivileged\n",
    "\n",
    "def compute_m(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Compute number of instances M to relabel.\n",
    "    disc = compute_discrimination(df, sensitive_attr, class_attr, privileged_value, positive_class)\n",
    "    \n",
    "    n_privileged = len(df[df[sensitive_attr] == privileged_value])\n",
    "    n_unprivileged = len(df[df[sensitive_attr] != privileged_value])\n",
    "    \n",
    "    return int(disc * (n_privileged * n_unprivileged) / len(df))\n",
    "\n",
    "def rank_instances(df, features, sensitive_attr, class_attr):\n",
    "    # Train a classifier to rank instances by likelihood of being positive.\n",
    "    X = df[features]\n",
    "    y = df[class_attr]\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    scores = model.predict_proba(X)[:, 1]  # Probability of positive class\n",
    "    df['score'] = scores\n",
    "    return df\n",
    "\n",
    "def apply_massaging(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Perform massaging technique.\n",
    "    # Step 1: Compute M\n",
    "    M = compute_m(df, sensitive_attr, class_attr, privileged_value, positive_class)\n",
    "    print(f\"Number of label changes (M): {M}\")\n",
    "\n",
    "    if M == 0:\n",
    "        print(\"No massaging needed.\")\n",
    "        return df\n",
    "\n",
    "    # Step 2: Rank instances\n",
    "    features = [col for col in df.columns if col not in [sensitive_attr, class_attr]]\n",
    "    df = rank_instances(df, features, sensitive_attr, class_attr)\n",
    "\n",
    "    # Step 3: Modify labels\n",
    "    unprivileged_neg = df[(df[sensitive_attr] != privileged_value) & (df[class_attr] != positive_class)]\n",
    "    privileged_pos = df[(df[sensitive_attr] == privileged_value) & (df[class_attr] == positive_class)]\n",
    "\n",
    "    # Promote top M from unprivileged_neg\n",
    "    df.loc[unprivileged_neg.nlargest(M, 'score').index, class_attr] = positive_class\n",
    "\n",
    "    # Demote bottom M from privileged_pos\n",
    "    df.loc[privileged_pos.nsmallest(M, 'score').index, class_attr] = 1 - positive_class\n",
    "\n",
    "    # Drop the ranking column\n",
    "    df.drop(columns=['score'], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8043\n",
      "Precision: 0.6587\n",
      "Recall: 0.3781\n",
      "F1 Score: 0.4804\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.88      7431\n",
      "         1.0       0.66      0.38      0.48      2338\n",
      "\n",
      "    accuracy                           0.80      9769\n",
      "   macro avg       0.74      0.66      0.68      9769\n",
      "weighted avg       0.79      0.80      0.78      9769\n",
      "\n",
      "Fairness Metrics:\n",
      "\n",
      "1. Individual Fairness For Baseline: 0.7540\n",
      "2. Disparate Impact Score for Baseline: 0.0000\n",
      "3. Counterfactual Fairness Score For Baseline: 0.8063\n",
      "4. Equality of Opportunity (EO) for Baseline: 0.4471\n",
      "5. Equality of Odds (EOd) for Baseline: 0.5482\n",
      "6. Group-Level Fairness Metrics After Massaging:\n",
      "\tStatistical Parity Difference: -0.2061\n",
      "\tDisparate Impact: 0.0000\n",
      "\tDemographic Parity: -0.2061\n"
     ]
    }
   ],
   "source": [
    "scale_orig = StandardScaler()\n",
    "lmod = LogisticRegression()\n",
    "\n",
    "x_orig = df_original.drop(columns=['Income Binary'])\n",
    "y_orig = df_original['Income Binary']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "\n",
    "# Scale numerical features for training\n",
    "x_train = scale_orig.fit_transform(x_train)\n",
    "\n",
    "# Train the model\n",
    "lmod.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "x_test_scaled = scale_orig.fit_transform(x_test)\n",
    "y_pred = lmod.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate performance of baseline model\n",
    "eval_performance(y_test, y_pred)\n",
    "\n",
    "# ---- Fairness Assessment ----\n",
    "\n",
    "print(\"Fairness Metrics:\\n\")\n",
    "\n",
    "# Evaluate Individual Fairness\n",
    "ind_fairness_orig = eval_ind_fairness(x_train, y_train, x_test_scaled, y_pred)\n",
    "print(f'1. Individual Fairness For Baseline: {ind_fairness_orig:.4f}')\n",
    "\n",
    "# Evaluate Disparate Impact\n",
    "disparate_impact_orig = eval_disparate_impact(x_test['sex'], y_pred)\n",
    "print(f'2. Disparate Impact Score for Baseline: {disparate_impact_orig:.4f}')\n",
    "\n",
    "# Evaluate Counterfactual Fairness\n",
    "counterfactual_fairness_orig = evaluate_counterfactual_fairness_sex(lmod, x_test)\n",
    "fairness_metric_sex = counterfactual_fairness_orig['same_decision'].mean()\n",
    "print(f'3. Counterfactual Fairness Score For Baseline: {fairness_metric_sex:.4f}')\n",
    "\n",
    "# Evaluate Equality of Opportunity and Equality of Odds\n",
    "tpr_diff_orig, eod_orig = eval_equality(x_test_scaled, y_pred, sensitive_attribute_index = 1, y_test=y_test) # Column 1 holds the 'sex' attribute in x_test_scaled dataframe\n",
    "print(f\"4. Equality of Opportunity (EO) for Baseline: {tpr_diff_orig:.4f}\")\n",
    "print(f\"5. Equality of Odds (EOd) for Baseline: {eod_orig:.4f}\")\n",
    "\n",
    "# Evaluate Group-Level Fairness Metrics\n",
    "group_fairness_orig = eval_group_fairness(x_test, target='Income Binary', protected_attr='sex', mode='model', y_pred=y_pred)\n",
    "print(\"6. Group-Level Fairness Metrics After Massaging:\")\n",
    "for metric, value in group_fairness_orig.items():\n",
    "    print(f\"\\t{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance After Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label changes (M): 2105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8634\n",
      "Precision: 0.7290\n",
      "Recall: 0.6835\n",
      "F1 Score: 0.7055\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91      7431\n",
      "         1.0       0.73      0.68      0.71      2338\n",
      "\n",
      "    accuracy                           0.86      9769\n",
      "   macro avg       0.82      0.80      0.81      9769\n",
      "weighted avg       0.86      0.86      0.86      9769\n",
      "\n",
      "Fairness Metrics:\n",
      "\n",
      "1. Individual Fairness Ater Massaging: 0.8422\n",
      "2. Disparate Impact Score After Massaging: 0.8236\n",
      "3. Counterfactual Fairness Score After Massaging: 0.9818\n",
      "4. Equality of Opportunity (EO) After Massaging: 0.1581\n",
      "5. Equality of Odds (EOd) After Massaging: 0.2686\n",
      "6. Group-Level Fairness Metrics After Massaging:\n",
      "\tStatistical Parity Difference: -0.0421\n",
      "\tDisparate Impact: 0.8236\n",
      "\tDemographic Parity: -0.0421\n"
     ]
    }
   ],
   "source": [
    "# Apply massaging technique\n",
    "df_massaged = apply_massaging(df_original, 'sex', 'Income Binary', privileged_value=1, positive_class=1)\n",
    "\n",
    "# Repeat training and testing with massaged data\n",
    "x_massaged = df_massaged.drop(columns=['Income Binary'])\n",
    "y_massaged = df_massaged['Income Binary']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_massaged, y_massaged, test_size=0.2, random_state=42, stratify=y_massaged)\n",
    "\n",
    "# Scale numerical features for training\n",
    "x_train = scale_orig.fit_transform(x_train)\n",
    "\n",
    "# Train the model\n",
    "lmod.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "x_test_scaled = scale_orig.fit_transform(x_test)\n",
    "y_pred = lmod.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate performance of baseline model\n",
    "eval_performance(y_test, y_pred)\n",
    "\n",
    "# ---- Fairness Assessment ----\n",
    "\n",
    "print(\"Fairness Metrics:\\n\")\n",
    "\n",
    "# Evaluate Individual Fairness\n",
    "ind_fairness_massaged = eval_ind_fairness(x_train, y_train, x_test_scaled, y_pred)\n",
    "print(f'1. Individual Fairness Ater Massaging: {ind_fairness_massaged:.4f}')\n",
    "\n",
    "# Evaluate Disparate Impact\n",
    "disparate_impact_massaged = eval_disparate_impact(x_test['sex'], y_pred) # x_test[:,1] is the column for the 'sex' attribute\n",
    "print(f'2. Disparate Impact Score After Massaging: {disparate_impact_massaged:.4f}')\n",
    "\n",
    "# Evaluate Counterfactual Fairness\n",
    "counterfactual_fairness_massaged = evaluate_counterfactual_fairness_sex(lmod, x_test)\n",
    "fairness_metric_sex = counterfactual_fairness_massaged['same_decision'].mean()\n",
    "print(f'3. Counterfactual Fairness Score After Massaging: {fairness_metric_sex:.4f}')\n",
    "\n",
    "# Evaluate Equality of Opportunity and Equality of Odds\n",
    "tpr_diff_massaged, eod_orig_massaged = eval_equality(x_test_scaled, y_pred, sensitive_attribute_index = 1, y_test=y_test) # Column 1 holds the 'sex' attribute in x_test_scaled dataframe\n",
    "print(f\"4. Equality of Opportunity (EO) After Massaging: {tpr_diff_massaged:.4f}\")\n",
    "print(f\"5. Equality of Odds (EOd) After Massaging: {eod_orig_massaged:.4f}\")\n",
    "\n",
    "# Evaluate Group-Level Fairness Metrics\n",
    "group_fairness_massaged = eval_group_fairness(x_test, target='Income Binary', protected_attr='sex', mode='model', y_pred=y_pred)\n",
    "print(\"6. Group-Level Fairness Metrics After Massaging:\")\n",
    "for metric, value in group_fairness_massaged.items():\n",
    "    print(f\"\\t{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "This version of the massaging technique uses the same preprocessing as adult_reweighing.ipynb (from AIF360) for the baseline model. The baseline model's performance is nearly identical to that found in adult_reweighing.ipynb. \n",
    "\n",
    "## Results\n",
    "\n",
    "Massaging the data improved the model's predictive performance. This could be because the baseline model relies to heavily on the \"sex\" attribute. The model's individual fairness increased significantly after massaging. The disparate impact and counterfactual fairness of the model increased with massaging. The Equality of Opportuniy and Equality of Odds are lowered after massaging, indicating the model is fair in predicting both positive and negative outcomes for both sexes. For group-level fairness metrics, the statistical parity difference and demographic parity has decreased. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
