{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massaging to Improve Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Creating the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, classification_report, recall_score, f1_score\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult\n",
    "\n",
    "# Import files for Fairness Metrics\n",
    "from individual_fairness import eval_ind_fairness \n",
    "from disparate_impact import eval_disparate_impact\n",
    "from counterfactual_fairness import evaluate_counterfactual_fairness_sex\n",
    "from equality_fairness import eval_equality\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "dataset_orig = load_preproc_data_adult(['sex'])\n",
    "np.random.seed(42)\n",
    "\n",
    "# Convert the Dataset into a Dataframe for easier massaging\n",
    "df_original = pd.DataFrame(columns=dataset_orig.feature_names, data=dataset_orig.features)\n",
    "df_original['Income Binary'] = dataset_orig.labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_performance(y_test, y_pred):\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Display metrics\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masaging to Remove Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discrimination(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Compute the discrimination score (difference in positive outcome rates).\n",
    "    privileged = df[df[sensitive_attr] == privileged_value]\n",
    "    unprivileged = df[df[sensitive_attr] != privileged_value]\n",
    "\n",
    "    pos_rate_privileged = sum(privileged[class_attr] == positive_class) / len(privileged)\n",
    "    pos_rate_unprivileged = sum(unprivileged[class_attr] == positive_class) / len(unprivileged)\n",
    "\n",
    "    return pos_rate_privileged - pos_rate_unprivileged\n",
    "\n",
    "def compute_m(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Compute number of instances M to relabel.\n",
    "    disc = compute_discrimination(df, sensitive_attr, class_attr, privileged_value, positive_class)\n",
    "    \n",
    "    n_privileged = len(df[df[sensitive_attr] == privileged_value])\n",
    "    n_unprivileged = len(df[df[sensitive_attr] != privileged_value])\n",
    "    \n",
    "    return int(disc * (n_privileged * n_unprivileged) / len(df))\n",
    "\n",
    "def rank_instances(df, features, sensitive_attr, class_attr):\n",
    "    # Train a classifier to rank instances by likelihood of being positive.\n",
    "    X = df[features]\n",
    "    y = df[class_attr]\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    scores = model.predict_proba(X)[:, 1]  # Probability of positive class\n",
    "    df['score'] = scores\n",
    "    return df\n",
    "\n",
    "def apply_massaging(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Perform massaging technique.\n",
    "    # Step 1: Compute M\n",
    "    M = compute_m(df, sensitive_attr, class_attr, privileged_value, positive_class)\n",
    "    print(f\"Number of label changes (M): {M}\")\n",
    "\n",
    "    if M == 0:\n",
    "        print(\"No massaging needed.\")\n",
    "        return df\n",
    "\n",
    "    # Step 2: Rank instances\n",
    "    features = [col for col in df.columns if col not in [sensitive_attr, class_attr]]\n",
    "    df = rank_instances(df, features, sensitive_attr, class_attr)\n",
    "\n",
    "    # Step 3: Modify labels\n",
    "    unprivileged_neg = df[(df[sensitive_attr] != privileged_value) & (df[class_attr] != positive_class)]\n",
    "    privileged_pos = df[(df[sensitive_attr] == privileged_value) & (df[class_attr] == positive_class)]\n",
    "\n",
    "    # Promote top M from unprivileged_neg\n",
    "    df.loc[unprivileged_neg.nlargest(M, 'score').index, class_attr] = positive_class\n",
    "\n",
    "    # Demote bottom M from privileged_pos\n",
    "    df.loc[privileged_pos.nsmallest(M, 'score').index, class_attr] = 1 - positive_class\n",
    "\n",
    "    # Drop the ranking column\n",
    "    df.drop(columns=['score'], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_counterfactual_fairness_sex(model, X):\n",
    "    \"\"\"\n",
    "    Evaluates counterfactual fairness by flipping the 'sex' attribute.\n",
    "    \"\"\"\n",
    "    # Store the original predictions on the test set\n",
    "    original_predictions = model.predict(StandardScaler().fit_transform(X))\n",
    "\n",
    "    # Create a copy of the dataset\n",
    "    X_counterfactual = X.copy()\n",
    "\n",
    "    # Flip 'sex' (0 -> 1, 1 -> 0)\n",
    "    if 'sex' in X.columns:\n",
    "        X_counterfactual['sex'] = 1 - X_counterfactual['sex']\n",
    "    else:\n",
    "        raise KeyError(f\"The sensitive attribute 'sex' is not present in the dataset.\")\n",
    "\n",
    "    # Get counterfactual predictions\n",
    "    counterfactual_predictions = model.predict(StandardScaler().fit_transform(X_counterfactual))\n",
    "\n",
    "    # Compare the original and counterfactual predictions\n",
    "    comparison = pd.DataFrame({\n",
    "        'original': original_predictions,\n",
    "        'counterfactual': counterfactual_predictions,\n",
    "        'same_decision': original_predictions == counterfactual_predictions\n",
    "    })\n",
    "\n",
    "    # Return the comparison dataframe\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8043\n",
      "Precision: 0.6587\n",
      "Recall: 0.3781\n",
      "F1 Score: 0.4804\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.88      7431\n",
      "         1.0       0.66      0.38      0.48      2338\n",
      "\n",
      "    accuracy                           0.80      9769\n",
      "   macro avg       0.74      0.66      0.68      9769\n",
      "weighted avg       0.79      0.80      0.78      9769\n",
      "\n",
      "Individual Fairness For Baseline: 0.7947\n",
      "Disparate Impact Score for Baseline: 0.0000\n",
      "Counterfactual Fairness Score For Baseline: 0.8063\n"
     ]
    }
   ],
   "source": [
    "scale_orig = StandardScaler()\n",
    "lmod = LogisticRegression()\n",
    "\n",
    "x_orig = df_original.drop(columns=['Income Binary'])\n",
    "y_orig = df_original['Income Binary']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "\n",
    "# Scale numerical features for training\n",
    "x_train = scale_orig.fit_transform(x_train)\n",
    "\n",
    "# Train the model\n",
    "lmod.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "x_test_scaled = scale_orig.fit_transform(x_test)\n",
    "y_pred = lmod.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate performance of baseline model\n",
    "eval_performance(y_test, y_pred)\n",
    "\n",
    "# Evaluate Individual Fairness\n",
    "ind_fairness_orig = eval_ind_fairness(x_train, y_train, x_test_scaled, y_pred)\n",
    "print(f'Individual Fairness For Baseline: {ind_fairness_orig:.4f}')\n",
    "\n",
    "# Evaluate Disparate Impact\n",
    "disparate_impact_orig = eval_disparate_impact(x_test['sex'], y_pred)\n",
    "print(f'Disparate Impact Score for Baseline: {disparate_impact_orig:.4f}')\n",
    "\n",
    "# Evaluate Counterfactual Fairness\n",
    "counterfactual_fairness = evaluate_counterfactual_fairness_sex(lmod, x_test)\n",
    "fairness_metric_sex = counterfactual_fairness['same_decision'].mean()\n",
    "print(f'Counterfactual Fairness Score For Baseline: {fairness_metric_sex:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equality of Opportunity (EO): 0.4471421345472939\n",
      "Equality of Odds (EOd): 0.548178975491481\n"
     ]
    }
   ],
   "source": [
    "sensitive_attribute_index = 1 # index of the 'sex' column in x_test_scaled\n",
    "tpr_diff, eod = eval_equality(x_test_scaled, y_pred, sensitive_attribute_index, y_test)\n",
    "\n",
    "print(\"Equality of Opportunity (EO):\", tpr_diff)\n",
    "print(\"Equality of Odds (EOd):\", eod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance After Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label changes (M): 2105\n",
      "Accuracy: 0.8645\n",
      "Precision: 0.7367\n",
      "Recall: 0.6749\n",
      "F1 Score: 0.7045\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91      7431\n",
      "         1.0       0.74      0.67      0.70      2338\n",
      "\n",
      "    accuracy                           0.86      9769\n",
      "   macro avg       0.82      0.80      0.81      9769\n",
      "weighted avg       0.86      0.86      0.86      9769\n",
      "\n",
      "Individual Fairness Ater Massaging: 0.8892\n",
      "Disparate Impact Score After Massaging: 0.8512\n",
      "Counterfactual Fairness Score After Massaging: 0.9767\n"
     ]
    }
   ],
   "source": [
    "# Apply massaging technique\n",
    "df_massaged = apply_massaging(df_original, 'sex', 'Income Binary', privileged_value=1, positive_class=1)\n",
    "\n",
    "# Repeat training and testing with massaged data\n",
    "x_massaged = df_massaged.drop(columns=['Income Binary'])\n",
    "y_massaged = df_massaged['Income Binary']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_massaged, y_massaged, test_size=0.2, random_state=42, stratify=y_massaged)\n",
    "\n",
    "# Scale numerical features for training\n",
    "x_train = scale_orig.fit_transform(x_train)\n",
    "\n",
    "# Train the model\n",
    "lmod.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "x_test_scaled = scale_orig.fit_transform(x_test)\n",
    "y_pred = lmod.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate performance of baseline model\n",
    "eval_performance(y_test, y_pred)\n",
    "\n",
    "# Evaluate Individual Fairness\n",
    "ind_fairness_massaged = eval_ind_fairness(x_train, y_train, x_test_scaled, y_pred)\n",
    "print(f'Individual Fairness Ater Massaging: {ind_fairness_massaged:.4f}')\n",
    "\n",
    "# # Evaluate Disparate Impact\n",
    "disparate_impact_massaged = eval_disparate_impact(x_test['sex'], y_pred) # x_test[:,1] is the column for the 'sex' attribute\n",
    "print(f'Disparate Impact Score After Massaging: {disparate_impact_massaged:.4f}')\n",
    "\n",
    "# # Evaluate Counterfactual Fairness\n",
    "counterfactual_fairness = evaluate_counterfactual_fairness_sex(lmod, x_test)\n",
    "fairness_metric_sex = counterfactual_fairness['same_decision'].mean()\n",
    "print(f'Counterfactual Fairness Score After Massaging: {fairness_metric_sex:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equality of Opportunity (EO) After Massaging: 0.17111508452535762\n",
      "Equality of Odds (EOd) After Massaging: 0.27554806422382366\n"
     ]
    }
   ],
   "source": [
    "sensitive_attribute_index = 1  # Set the correct index of the 'sex' column in x_test_scaled\n",
    "tpr_diff, eod = eval_equality(x_test_scaled, y_pred, sensitive_attribute_index, y_test)\n",
    "\n",
    "print(\"Equality of Opportunity (EO) After Massaging:\", tpr_diff)\n",
    "print(\"Equality of Odds (EOd) After Massaging:\", eod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "This version of the massaging technique uses the same preprocessing as adult_reweighing.ipynb (from AIF360) for the baseline model. The baseline model's performance is nearly identical to that found in adult_reweighing.ipynb. \n",
    "\n",
    "## Results\n",
    "\n",
    "Massaging the data improved the model's predictive performance. This could be because the baseline model relies to heavily on the \"sex\" attribute. The model's individual fairness increased significantly after massaging. The disparate impact and counterfactual fairness of the model increased with massaging. The Equality of Opportuniy and Equality of Odds are lowered after massaging indicating the model is fair in predicting both positive and negative outcomes for both sexes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
