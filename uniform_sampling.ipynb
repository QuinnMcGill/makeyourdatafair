{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20feb1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/aif360/algorithms/preprocessing/optim_preproc_helpers/data_preproc_functions.py:50: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sex'] = df['sex'].replace({'Female': 0.0, 'Male': 1.0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "BEFORE UNIFORM SAMPLING (ORIGINAL DATA)\n",
      "========================================\n",
      "\n",
      "Dataset Group Fairness Metrics (Original):\n",
      "Statistical Parity Difference: -0.1945\n",
      "Disparate Impact: 0.3597\n",
      "Demographic Parity: -0.1945\n",
      "\n",
      "Model Performance (Original):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.88      7431\n",
      "         1.0       0.66      0.38      0.48      2338\n",
      "\n",
      "    accuracy                           0.80      9769\n",
      "   macro avg       0.74      0.66      0.68      9769\n",
      "weighted avg       0.79      0.80      0.78      9769\n",
      "\n",
      "\n",
      "Model Group Fairness Metrics (Original):\n",
      "Statistical Parity Difference: -0.2061\n",
      "Disparate Impact: 0.0000\n",
      "Demographic Parity: -0.2061\n",
      "\n",
      "Individual Fairness Score (Original): 0.7562\n",
      "\n",
      "Equality of Opportunity (Original): 0.4471\n",
      "Equality of Odds (Original): 0.5482\n",
      "\n",
      "Disparate Impact (Original): 0.0000\n",
      "\n",
      "Counterfactual Fairness Score (Original): 0.8063\n",
      "\n",
      "========================================\n",
      "AFTER UNIFORM SAMPLING\n",
      "========================================\n",
      "\n",
      "Dataset Group Fairness Metrics (After Sampling):\n",
      "Statistical Parity Difference: 0.0000\n",
      "Disparate Impact: 1.0000\n",
      "Demographic Parity: 0.0000\n",
      "\n",
      "Model Performance (After Sampling):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.67      0.70       708\n",
      "         1.0       0.70      0.77      0.73       708\n",
      "\n",
      "    accuracy                           0.72      1416\n",
      "   macro avg       0.72      0.72      0.72      1416\n",
      "weighted avg       0.72      0.72      0.72      1416\n",
      "\n",
      "\n",
      "Model Group Fairness Metrics (After Sampling):\n",
      "Statistical Parity Difference: 0.0048\n",
      "Disparate Impact: 1.0087\n",
      "Demographic Parity: 0.0048\n",
      "\n",
      "Individual Fairness Score (After Sampling): 0.7219\n",
      "\n",
      "Equality of Opportunity (After Sampling): 0.0237\n",
      "Equality of Odds (After Sampling): 0.0340\n",
      "\n",
      "Disparate Impact (After Sampling): 1.0087\n",
      "\n",
      "Counterfactual Fairness Score (After Sampling): 0.9668\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult\n",
    "\n",
    "from group_fairness import eval_group_fairness\n",
    "from individual_fairness import eval_ind_fairness\n",
    "from counterfactual_fairness import evaluate_counterfactual_fairness_sex\n",
    "from equality_fairness import eval_equality\n",
    "from disparate_impact import eval_disparate_impact\n",
    "\n",
    "# Load and Prepare Data\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads and prepares the Adult dataset using AIF360's pre-processing utility.\n",
    "    Converts the AIF360 BinaryLabelDataset into a pandas DataFrame for easier handling.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with features and binary income label.\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Load dataset using AIF360's pre-processing function (filters by 'sex')\n",
    "    dataset_orig = load_preproc_data_adult(['sex'])\n",
    "\n",
    "    # Convert the dataset to a DataFrame\n",
    "    df = pd.DataFrame(columns=dataset_orig.feature_names, data=dataset_orig.features)\n",
    "    df['Income Binary'] = dataset_orig.labels  # AIF360 uses labels, rename to match your other code\n",
    "\n",
    "    # Include protected attribute 'sex' explicitly\n",
    "    df['sex'] = dataset_orig.protected_attributes[:, 0]  # First (and only) protected attr: 'sex'\n",
    "\n",
    "    return df\n",
    "\n",
    "# Uniform Sampling Preprocessing\n",
    "def uniform_sampling(df, target, protected_attr):\n",
    "    groups = [\n",
    "        (df[protected_attr] == 0) & (df[target] == 0),  # Female, low income\n",
    "        (df[protected_attr] == 0) & (df[target] == 1),  # Female, high income\n",
    "        (df[protected_attr] == 1) & (df[target] == 0),  # Male, low income\n",
    "        (df[protected_attr] == 1) & (df[target] == 1)   # Male, high income\n",
    "    ]\n",
    "    \n",
    "    min_size = min([sum(g) for g in groups])\n",
    "    sampled_dfs = [df[g].sample(min_size, random_state=42) for g in groups]\n",
    "    return pd.concat(sampled_dfs)\n",
    "\n",
    "def main():\n",
    "    # Load preprocessed data using AIF360 format\n",
    "    df = load_data()\n",
    "\n",
    "    # =================================================================\n",
    "    # BEFORE SAMPLING (Original Data)\n",
    "    # =================================================================\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"BEFORE UNIFORM SAMPLING (ORIGINAL DATA)\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # 1. Dataset fairness (original)\n",
    "    orig_fairness = eval_group_fairness(df, 'Income Binary', 'sex', mode='dataset')\n",
    "    print(\"\\nDataset Group Fairness Metrics (Original):\")\n",
    "    for metric, value in orig_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # 2. Train model on original data\n",
    "    X_orig = df.drop(columns=['Income Binary'])\n",
    "    y_orig = df['Income Binary']\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig\n",
    "    )\n",
    "\n",
    "    scale_orig = StandardScaler()\n",
    "    X_train_scaled = scale_orig.fit_transform(X_train_orig)\n",
    "    X_test_scaled = scale_orig.transform(X_test_orig)\n",
    "\n",
    "    model_orig = LogisticRegression(max_iter=1000)\n",
    "    model_orig.fit(X_train_scaled, y_train_orig)\n",
    "\n",
    "    # 3. Model predictions\n",
    "    y_pred_orig = model_orig.predict(X_test_scaled)\n",
    "    print(\"\\nModel Performance (Original):\")\n",
    "    print(classification_report(y_test_orig, y_pred_orig))\n",
    "\n",
    "    # Group Model fairness on original dataset and model\n",
    "    orig_model_fairness = eval_group_fairness(X_test_orig, target='Income Binary', protected_attr='sex', mode='model', y_pred=y_pred_orig)\n",
    "    print(\"\\nModel Group Fairness Metrics (Original):\")\n",
    "    for metric, value in orig_model_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Evaluate individual fairness (before sampling)\n",
    "    ind_fairness_orig = eval_ind_fairness(X_train_scaled, y_train_orig, X_test_scaled, y_pred_orig)\n",
    "    print(f\"\\nIndividual Fairness Score (Original): {ind_fairness_orig:.4f}\")\n",
    "\n",
    "    # Evaluate Equality Fairness (Original)\n",
    "    sensitive_attr_idx_orig = X_test_orig.columns.get_loc('sex')  # Get index of 'sex' column\n",
    "    tpr_diff_orig, eod_orig = eval_equality(X_test_scaled, y_pred_orig, sensitive_attr_idx_orig, y_test_orig)\n",
    "    print(f\"\\nEquality of Opportunity (Original): {tpr_diff_orig:.4f}\")\n",
    "    print(f\"Equality of Odds (Original): {eod_orig:.4f}\")\n",
    "\n",
    "    # Disparate Impact (Original)\n",
    "    disparate_impact_orig = eval_disparate_impact(X_test_orig['sex'], y_pred_orig)\n",
    "    print(f\"\\nDisparate Impact (Original): {disparate_impact_orig:.4f}\")\n",
    "\n",
    "    # Evaluate Counterfactual Fairness (Original)\n",
    "    cf_results_orig = evaluate_counterfactual_fairness_sex(model_orig, X_test_orig)\n",
    "    cf_score_orig = cf_results_orig['same_decision'].mean()\n",
    "    print(f\"\\nCounterfactual Fairness Score (Original): {cf_score_orig:.4f}\")\n",
    "\n",
    "    # =================================================================\n",
    "    # AFTER SAMPLING\n",
    "    # =================================================================\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"AFTER UNIFORM SAMPLING\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # 5. Uniform sampling\n",
    "    df_sampled = uniform_sampling(df, 'Income Binary', 'sex')\n",
    "\n",
    "    # 6. Dataset fairness (after sampling)\n",
    "    sampled_fairness = eval_group_fairness(df_sampled, 'Income Binary', 'sex', mode='dataset')\n",
    "    print(\"\\nDataset Group Fairness Metrics (After Sampling):\")\n",
    "    for metric, value in sampled_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # 7. Train model on sampled data\n",
    "    X_sampled = df_sampled.drop(columns=['Income Binary'])\n",
    "    y_sampled = df_sampled['Income Binary']\n",
    "    X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = train_test_split(\n",
    "        X_sampled, y_sampled, test_size=0.2, random_state=42, stratify=y_sampled\n",
    "    )\n",
    "\n",
    "    X_train_sampled_scaled = scale_orig.fit_transform(X_train_sampled)\n",
    "    X_test_sampled_scaled = scale_orig.transform(X_test_sampled)\n",
    "\n",
    "    model_sampled = LogisticRegression(max_iter=1000)\n",
    "    model_sampled.fit(X_train_sampled_scaled, y_train_sampled)\n",
    "\n",
    "    # 8. Model performance & fairness (after sampling)\n",
    "    y_pred_sampled = model_sampled.predict(X_test_sampled_scaled)\n",
    "    print(\"\\nModel Performance (After Sampling):\")\n",
    "    print(classification_report(y_test_sampled, y_pred_sampled))\n",
    "\n",
    "    X_test_eval_sampled = X_test_sampled.copy()\n",
    "    X_test_eval_sampled['Income Binary'] = y_test_sampled\n",
    "    sampled_model_fairness = eval_group_fairness(\n",
    "        X_test_eval_sampled, target='Income Binary', protected_attr='sex', mode='model', y_pred=y_pred_sampled\n",
    "    )\n",
    "    print(\"\\nModel Group Fairness Metrics (After Sampling):\")\n",
    "    for metric, value in sampled_model_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Evaluate individual fairness (after sampling)\n",
    "    ind_fairness_sampled = eval_ind_fairness(X_train_sampled_scaled, y_train_sampled, X_test_sampled_scaled, y_pred_sampled)\n",
    "    print(f\"\\nIndividual Fairness Score (After Sampling): {ind_fairness_sampled:.4f}\")\n",
    "\n",
    "    # Evaluate Equality Fairness (After Sampling)\n",
    "    sensitive_attr_idx_sampled = X_test_sampled.columns.get_loc('sex')  # Get index of 'sex' column\n",
    "    tpr_diff_sampled, eod_sampled = eval_equality(X_test_sampled_scaled, y_pred_sampled, sensitive_attr_idx_sampled, y_test_sampled)\n",
    "    print(f\"\\nEquality of Opportunity (After Sampling): {tpr_diff_sampled:.4f}\")\n",
    "    print(f\"Equality of Odds (After Sampling): {eod_sampled:.4f}\")\n",
    "\n",
    "    # Disparate Impact (After Sampling)\n",
    "    disparate_impact_sampled = eval_disparate_impact(X_test_sampled['sex'], y_pred_sampled)\n",
    "    print(f\"\\nDisparate Impact (After Sampling): {disparate_impact_sampled:.4f}\")\n",
    "\n",
    "    # Evaluate Counterfactual Fairness (After Sampling)\n",
    "    cf_results_sampled = evaluate_counterfactual_fairness_sex(model_sampled, X_test_sampled)\n",
    "    cf_score_sampled = cf_results_sampled['same_decision'].mean()\n",
    "    print(f\"\\nCounterfactual Fairness Score (After Sampling): {cf_score_sampled:.4f}\")\n",
    "\n",
    "    # # =================================================================\n",
    "    # # IMPROVEMENT COMPARISON\n",
    "    # # =================================================================\n",
    "    # print(\"\\n\" + \"=\"*40)\n",
    "    # print(\"IMPROVEMENT COMPARISON\")\n",
    "    # print(\"=\"*40)\n",
    "\n",
    "    # print(\"\\nDataset Group Fairness Improvement:\")\n",
    "    # for metric in orig_fairness:\n",
    "    #     improvement = sampled_fairness[metric] - orig_fairness[metric]\n",
    "    #     print(f\"{metric}: {improvement:+.4f} (Before: {orig_fairness[metric]:.4f}, After: {sampled_fairness[metric]:.4f})\")\n",
    "\n",
    "    # print(\"\\nModel Group Fairness Improvement:\")\n",
    "    # for metric in orig_model_fairness:\n",
    "    #     improvement = sampled_model_fairness[metric] - orig_model_fairness[metric]\n",
    "    #     print(f\"{metric}: {improvement:+.4f} (Before: {orig_model_fairness[metric]:.4f}, After: {sampled_model_fairness[metric]:.4f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
