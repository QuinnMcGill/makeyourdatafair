{
  "cells": [
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "# Baseline Model for Census Income Dataset"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Part 1: Data Exploration"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 31,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "\n",
       "Number of Rows:  32560\n",
       "Number of Features:  15\n",
       "Numerical Features: ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
       "Categorical Features: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'income']\n"
      ]
     }
    ],
    "source": [
     "import pandas as pd\n",
     "import os\n",
     "import numpy as np\n",
     "from sklearn.model_selection import train_test_split\n",
     "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
     "from sklearn.compose import ColumnTransformer\n",
     "from sklearn.pipeline import Pipeline\n",
     "from sklearn.linear_model import LogisticRegression\n",
     "from sklearn.metrics import precision_score, accuracy_score, classification_report, recall_score, f1_score\n",
     "\n",
     "# Load the CSV file into a DataFrame\n",
     "file_path = os.path.join(\"..\", \"data\", \"income\", \"adult.data\") # Replace with your actual file path\n",
     "df = pd.read_csv(file_path)\n",
     "\n",
     "# Define column names\n",
     "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', \n",
     "                'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
     "                'hours-per-week', 'native-country', 'income']\n",
     "\n",
     "# Assign column names to the DataFrame\n",
     "df.columns = column_names\n",
     "\n",
     "# Display the first few rows of the DataFrame\n",
     "#print(df.head(4))\n",
     "print(\"\\nNumber of Rows: \", df.shape[0])\n",
     "print(\"Number of Features: \", df.shape[1])\n",
     "\n",
     "# Divide the features into numerical and non-numerical lists\n",
     "# Extract numerical and string features\n",
     "num_features = df.select_dtypes(include=['number']).columns.tolist()\n",
     "cat_features = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
     "\n",
     "# Display the feature lists\n",
     "print(\"Numerical Features:\", num_features)\n",
     "print(\"Categorical Features:\", cat_features)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 32,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Convert income column to binary, flag errors\n",
     "def convert_income(value):\n",
     "    value = str(value).strip()\n",
     "    if value == '>50K':\n",
     "        return 1\n",
     "    elif value == '<=50K':\n",
     "        return 0\n",
     "    else:\n",
     "        return np.nan  # Flag invalid values as NaN (or set a custom error flag)\n",
     "\n",
     "df['income'] = df['income'].apply(convert_income)\n",
     "\n",
     "# Identify and display rows with errors\n",
     "error_rows = df[df['income'].isna()]\n",
     "if not error_rows.empty:\n",
     "    print(\"Invalid income values found in \", error_rows.size, \"rows: \")\n",
     "    print(error_rows)\n",
     "\n",
     "\n",
     "df.head(4)\n",
     "cat_features.remove('income')"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Part 2: Baseline Model"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Define Helper Methods"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 33,
    "metadata": {},
    "outputs": [],
    "source": [
     "def eval_performance(y_test, y_pred):\n",
     "    # Evaluate performance\n",
     "    accuracy = accuracy_score(y_test, y_pred)\n",
     "    precision = precision_score(y_test, y_pred)\n",
     "    recall = recall_score(y_test, y_pred)\n",
     "    f1 = f1_score(y_test, y_pred)\n",
     "\n",
     "    # Display metrics\n",
     "    print(f'Accuracy: {accuracy:.4f}')\n",
     "    print(f'Precision: {precision:.4f}')\n",
     "    print(f'Recall: {recall:.4f}')\n",
     "    print(f'F1 Score: {f1:.4f}')\n",
     "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Train and Test Baseline Model"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 34,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Accuracy: 0.8621\n",
       "Precision: 0.7642\n",
       "Recall: 0.6180\n",
       "F1 Score: 0.6834\n",
       "\n",
       "Classification Report:\n",
       "               precision    recall  f1-score   support\n",
       "\n",
       "           0       0.89      0.94      0.91      4944\n",
       "           1       0.76      0.62      0.68      1568\n",
       "\n",
       "    accuracy                           0.86      6512\n",
       "   macro avg       0.82      0.78      0.80      6512\n",
       "weighted avg       0.86      0.86      0.86      6512\n",
       "\n"
      ]
     }
    ],
    "source": [
     "# Separate features and target variable\n",
     "X = df[num_features + cat_features]\n",
     "y = df['income']\n",
     "\n",
     "# Preprocessing: Standardize numerical features and one-hot encode categorical features\n",
     "preprocessor = ColumnTransformer([\n",
     "    ('num', StandardScaler(), num_features),\n",
     "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
     "])\n",
     "\n",
     "# Create a pipeline with logistic regression\n",
     "model = Pipeline([\n",
     "    ('preprocessor', preprocessor),\n",
     "    ('classifier', LogisticRegression(max_iter=1000))\n",
     "])\n",
     "\n",
     "# Split the data into training and test sets\n",
     "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
     "\n",
     "# Train the model\n",
     "model.fit(X_train, y_train)\n",
     "\n",
     "# Make predictions\n",
     "y_pred = model.predict(X_test)\n",
     "\n",
     "# Evaluate performance of baseline model\n",
     "eval_performance(y_test, y_pred)\n"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Part 3: Characterizing Fairness"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "#### Individual Fairness: \n",
     "\n",
     "Ensures that similar individuals receive similar predictions. A common way to assess this is by checking prediction consistency across similar instances. We can do this using a Nearest Neighbors Consistency Test."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Individual Fairness Consistency Score (with categorical features): 0.8276\n"
      ]
     }
    ],
    "source": [
     "from sklearn.neighbors import NearestNeighbors\n",
     "import gower\n",
     "\n",
     "# ---  HEADS UP!! This worked, but took 8 mins last time ---\n",
     "# I created a flag to stop it from always running.\n",
     "# --- RESULTS ----\n",
     "# Baseline model: 0.8276 with k = 5\n",
     "\n",
     "ind_fairness_flag = False\n",
     "\n",
     "if ind_fairness_flag == True:\n",
     "    # Scale numerical features & One-Hot Encode categorical features\n",
     "    preprocessor = ColumnTransformer([\n",
     "        ('num', StandardScaler(), num_features),\n",
     "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)\n",
     "    ])\n",
     "\n",
     "    # Transform training and test features\n",
     "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
     "    X_test_transformed = preprocessor.transform(X_test)\n",
     "\n",
     "    # Compute Gower distance matrix for test samples w.r.t training data\n",
     "    gower_distances = gower.gower_matrix(X_test, X_train)  # Shape: (num_test_samples, num_train_samples)\n",
     "\n",
     "    # Find k nearest neighbors (excluding self)\n",
     "    k = 5  # Adjust as needed\n",
     "    neighbors = np.argsort(gower_distances, axis=1)[:, 1:k+1]  # Get indices of k nearest neighbors\n",
     "\n",
     "    # Get model predictions\n",
     "    y_test_pred = model.predict(X_test)\n",
     "\n",
     "    # Compute consistency score: Fraction of nearest neighbors with same prediction\n",
     "    consistencies = []\n",
     "    for i, neigh_indices in enumerate(neighbors):\n",
     "        neighbor_preds = y_train.iloc[neigh_indices]  # Get predictions of k neighbors from training labels\n",
     "        consistency = np.mean(neighbor_preds == y_test_pred[i])  # Fraction with same prediction\n",
     "        consistencies.append(consistency)\n",
     "\n",
     "    # Calculate overall consistency score\n",
     "    individual_fairness_score = np.mean(consistencies)\n",
     "\n",
     "    print(f'Individual Fairness Consistency Score (with categorical features): {individual_fairness_score:.4f}')"
    ]
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.13.2"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 2
 }
 