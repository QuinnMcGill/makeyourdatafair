{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad659606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "BEFORE REWEIGHING (ORIGINAL DATA)\n",
      "========================================\n",
      "\n",
      "Dataset Fairness Metrics (Original):\n",
      "Statistical Parity Difference: -0.1963\n",
      "Disparate Impact: 0.3580\n",
      "\n",
      "Model Performance (Original):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90      4976\n",
      "           1       0.72      0.58      0.64      1537\n",
      "\n",
      "    accuracy                           0.85      6513\n",
      "   macro avg       0.80      0.76      0.77      6513\n",
      "weighted avg       0.84      0.85      0.84      6513\n",
      "\n",
      "\n",
      "Model Fairness Metrics (Original):\n",
      "Equal Opportunity Difference: -0.0777\n",
      "Average Odds Difference: -0.0781\n",
      "Disparate Impact (Predictions): 0.3051\n",
      "\n",
      "========================================\n",
      "AFTER REWEIGHING\n",
      "========================================\n",
      "\n",
      "Dataset Fairness Metrics (After Reweighing):\n",
      "Statistical Parity Difference: -0.1963\n",
      "Disparate Impact: 0.3580\n",
      "\n",
      "Model Performance (After Reweighing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91      4976\n",
      "           1       0.74      0.56      0.64      1537\n",
      "\n",
      "    accuracy                           0.85      6513\n",
      "   macro avg       0.81      0.75      0.77      6513\n",
      "weighted avg       0.84      0.85      0.84      6513\n",
      "\n",
      "\n",
      "Model Fairness Metrics (After Reweighing):\n",
      "Equal Opportunity Difference: 0.1465\n",
      "Average Odds Difference: 0.0639\n",
      "Disparate Impact (Predictions): 0.5671\n",
      "\n",
      "========================================\n",
      "IMPROVEMENT COMPARISON\n",
      "========================================\n",
      "\n",
      "Dataset Fairness Improvement:\n",
      "Statistical Parity Difference: +0.0000 (Before: -0.1963, After: -0.1963)\n",
      "Disparate Impact: +0.0000 (Before: 0.3580, After: 0.3580)\n",
      "\n",
      "Model Fairness Improvement:\n",
      "Equal Opportunity Difference: +0.2242 (Before: -0.0777, After: 0.1465)\n",
      "Average Odds Difference: +0.1420 (Before: -0.0781, After: 0.0639)\n",
      "Disparate Impact (Predictions): +0.2620 (Before: 0.3051, After: 0.5671)\n"
     ]
    }
   ],
   "source": [
    "import aif360\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset, StandardDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "# 1. Load and Prepare Data (unchanged)\n",
    "def load_data():\n",
    "    import kagglehub\n",
    "    path = kagglehub.dataset_download(\"uciml/adult-census-income\")\n",
    "    df = pd.read_csv(path + '/adult.csv')\n",
    "    df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "    \n",
    "    # Convert target and protected attribute\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})\n",
    "    df['sex'] = df['sex'].map({'Female': 0, 'Male': 1})\n",
    "    \n",
    "    # Convert other categorical columns to numerical codes\n",
    "    categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', \n",
    "                       'relationship', 'race', 'native-country']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def manual_reweighing(df, target, protected_attr):\n",
    "    \"\"\"\n",
    "    Reweighing implementation following the exact theoretical formulation:\n",
    "    1. Calculate expected probability Pexp(S=s, Class=c) = P(S=s) * P(Class=c)\n",
    "    2. Calculate observed probability Pobs(S=s, Class=c)\n",
    "    3. Compute weights W(X) = Pexp / Pobs\n",
    "    4. Return reweighted dataset\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate expected probabilities (assuming independence)\n",
    "    total = len(df)\n",
    "    p_s1 = len(df[df[protected_attr] == 1]) / total  # P(S=1)\n",
    "    p_s0 = 1 - p_s1                                  # P(S=0)\n",
    "    p_c1 = len(df[df[target] == 1]) / total          # P(Class=1)\n",
    "    p_c0 = 1 - p_c1                                  # P(Class=0)\n",
    "    \n",
    "    # Expected probabilities for each combination\n",
    "    expected_probs = {\n",
    "        (1, 1): p_s1 * p_c1,\n",
    "        (1, 0): p_s1 * p_c0,\n",
    "        (0, 1): p_s0 * p_c1,\n",
    "        (0, 0): p_s0 * p_c0\n",
    "    }\n",
    "    \n",
    "    # Step 2: Calculate observed probabilities\n",
    "    observed_counts = {\n",
    "        (1, 1): len(df[(df[protected_attr] == 1) & (df[target] == 1)]),\n",
    "        (1, 0): len(df[(df[protected_attr] == 1) & (df[target] == 0)]),\n",
    "        (0, 1): len(df[(df[protected_attr] == 0) & (df[target] == 1)]),\n",
    "        (0, 0): len(df[(df[protected_attr] == 0) & (df[target] == 0)])\n",
    "    }\n",
    "    \n",
    "    observed_probs = {k: v/total for k, v in observed_counts.items()}\n",
    "    \n",
    "    # Step 3: Compute weights\n",
    "    weights = {}\n",
    "    for key in expected_probs:\n",
    "        if observed_probs[key] > 0:\n",
    "            weights[key] = expected_probs[key] / observed_probs[key]\n",
    "        else:\n",
    "            weights[key] = 0  # Handle division by zero\n",
    "    \n",
    "    # Step 4: Assign weights to each instance\n",
    "    df_reweighed = df.copy()\n",
    "    df_reweighed['weights'] = df.apply(\n",
    "        lambda row: weights[(row[protected_attr], row[target])], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df_reweighed\n",
    "\n",
    "# 3. Fairness Evaluation Utilities (unchanged)\n",
    "def evaluate_dataset_fairness(df, target, protected_attr):\n",
    "    \"\"\"Evaluate fairness at dataset level\"\"\"\n",
    "    dataset = BinaryLabelDataset(\n",
    "        df=df,\n",
    "        label_names=[target],\n",
    "        protected_attribute_names=[protected_attr]\n",
    "    )\n",
    "    metric = BinaryLabelDatasetMetric(\n",
    "        dataset,\n",
    "        unprivileged_groups=[{protected_attr: 0}],  # Female\n",
    "        privileged_groups=[{protected_attr: 1}]      # Male\n",
    "    )\n",
    "    return {\n",
    "        'Statistical Parity Difference': metric.statistical_parity_difference(),\n",
    "        'Disparate Impact': metric.disparate_impact()\n",
    "    }\n",
    "\n",
    "def evaluate_model_fairness(y_true, y_pred, protected_attr):\n",
    "    \"\"\"Evaluate fairness of model predictions\"\"\"\n",
    "    dataset_true = BinaryLabelDataset(\n",
    "        df=pd.DataFrame({'y_true': y_true, 'protected': protected_attr}),\n",
    "        label_names=['y_true'],\n",
    "        protected_attribute_names=['protected']\n",
    "    )\n",
    "    dataset_pred = dataset_true.copy()\n",
    "    dataset_pred.labels = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    metric = ClassificationMetric(\n",
    "        dataset_true,\n",
    "        dataset_pred,\n",
    "        unprivileged_groups=[{'protected': 0}],  # Female\n",
    "        privileged_groups=[{'protected': 1}]      # Male\n",
    "    )\n",
    "    return {\n",
    "        'Equal Opportunity Difference': metric.equal_opportunity_difference(),\n",
    "        'Average Odds Difference': metric.average_odds_difference(),\n",
    "        'Disparate Impact (Predictions)': BinaryLabelDatasetMetric(\n",
    "            dataset_pred,\n",
    "            unprivileged_groups=[{'protected': 0}],\n",
    "            privileged_groups=[{'protected': 1}]\n",
    "        ).disparate_impact()\n",
    "    }\n",
    "\n",
    "# 4. Main Pipeline (modified for reweighing)\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_data()\n",
    "    \n",
    "    # =================================================================\n",
    "    # BEFORE REWEIGHING (Original Data)\n",
    "    # =================================================================\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"BEFORE REWEIGHING (ORIGINAL DATA)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Evaluate original dataset fairness\n",
    "    orig_fairness = evaluate_dataset_fairness(df, 'income', 'sex')\n",
    "    print(\"\\nDataset Fairness Metrics (Original):\")\n",
    "    for metric, value in orig_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Train model on original data\n",
    "    X_orig = df.drop(columns=['income', 'fnlwgt'])\n",
    "    y_orig = df['income']\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X_orig, y_orig, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), \n",
    "         ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country'])\n",
    "    ])\n",
    "    \n",
    "    model_orig = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    model_orig.fit(X_train_orig, y_train_orig)\n",
    "    \n",
    "    # Evaluate original model\n",
    "    y_pred_orig = model_orig.predict(X_test_orig)\n",
    "    print(\"\\nModel Performance (Original):\")\n",
    "    print(classification_report(y_test_orig, y_pred_orig))\n",
    "    \n",
    "    orig_model_fairness = evaluate_model_fairness(y_test_orig, y_pred_orig, X_test_orig['sex'])\n",
    "    print(\"\\nModel Fairness Metrics (Original):\")\n",
    "    for metric, value in orig_model_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # AFTER REWEIGHING\n",
    "    # =================================================================\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"AFTER REWEIGHING\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Apply reweighing\n",
    "    df_reweighed = manual_reweighing(df, 'income', 'sex')\n",
    "    \n",
    "    # Evaluate reweighed dataset fairness\n",
    "    reweighed_fairness = evaluate_dataset_fairness(df_reweighed, 'income', 'sex')\n",
    "    print(\"\\nDataset Fairness Metrics (After Reweighing):\")\n",
    "    for metric, value in reweighed_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Train model on reweighed data\n",
    "    X_reweighed = df_reweighed.drop(columns=['income', 'fnlwgt', 'weights'])\n",
    "    y_reweighed = df_reweighed['income']\n",
    "    weights_reweighed = df_reweighed['weights']\n",
    "    \n",
    "    X_train_rw, X_test_rw, y_train_rw, y_test_rw, weights_train_rw, _ = train_test_split(\n",
    "        X_reweighed, y_reweighed, weights_reweighed, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    model_rw = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    model_rw.fit(\n",
    "        X_train_rw, \n",
    "        y_train_rw, \n",
    "        classifier__sample_weight=df_reweighed.loc[X_train_rw.index, 'weights']\n",
    "    )\n",
    "    \n",
    "    # Evaluate reweighed model\n",
    "    y_pred_rw = model_rw.predict(X_test_rw)\n",
    "    print(\"\\nModel Performance (After Reweighing):\")\n",
    "    print(classification_report(y_test_rw, y_pred_rw))\n",
    "    \n",
    "    # Use original test set's 'sex' for fairness evaluation (same random_state)\n",
    "    rw_model_fairness = evaluate_model_fairness(y_test_rw, y_pred_rw, X_test_orig['sex'])\n",
    "    print(\"\\nModel Fairness Metrics (After Reweighing):\")\n",
    "    for metric, value in rw_model_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # IMPROVEMENT COMPARISON\n",
    "    # =================================================================\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"IMPROVEMENT COMPARISON\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(\"\\nDataset Fairness Improvement:\")\n",
    "    for metric in orig_fairness:\n",
    "        improvement = reweighed_fairness[metric] - orig_fairness[metric]\n",
    "        print(f\"{metric}: {improvement:+.4f} (Before: {orig_fairness[metric]:.4f}, After: {reweighed_fairness[metric]:.4f})\")\n",
    "    \n",
    "    print(\"\\nModel Fairness Improvement:\")\n",
    "    for metric in orig_model_fairness:\n",
    "        improvement = rw_model_fairness[metric] - orig_model_fairness[metric]\n",
    "        print(f\"{metric}: {improvement:+.4f} (Before: {orig_model_fairness[metric]:.4f}, After: {rw_model_fairness[metric]:.4f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
