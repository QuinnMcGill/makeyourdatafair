{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the adult.data dataset (replace with the correct path)\n",
        "column_names = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
        "]\n",
        "\n",
        "# Load data (adjust path to the dataset as needed)\n",
        "df = pd.read_csv('adult.data', header=None, names=column_names, na_values=' ?', skipinitialspace=True)\n",
        "\n",
        "# Preprocessing: Convert categorical columns to numerical using One-Hot Encoding and handle protected attributes (sex, race)\n",
        "categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'native-country']\n",
        "\n",
        "# Convert 'sex' and 'race' to binary encoding\n",
        "df['sex'] = df['sex'].apply(lambda x: 1 if x.strip() == 'Male' else 0)\n",
        "df['race'] = df['race'].apply(lambda x: 1 if x.strip() == 'White' else 0)\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "df = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "# Convert target variable (income) to binary: 1 if '>50K' else 0\n",
        "df['income'] = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = df.drop(columns=['income'])\n",
        "y = df['income']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest model (no SMOTE)\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate overall model performance using ROC AUC\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f'Overall ROC AUC: {roc_auc:.4f}')\n",
        "\n",
        "# Calculate confusion matrix for the model\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "# Calculate TPR, FPR, FNR for the entire dataset\n",
        "TPR = tp / (tp + fn)  # True Positive Rate (Recall)\n",
        "FPR = fp / (fp + tn)  # False Positive Rate\n",
        "FNR = fn / (fn + tp)  # False Negative Rate\n",
        "print(f\"Overall TPR: {TPR:.4f}, FPR: {FPR:.4f}, FNR: {FNR:.4f}\")\n",
        "\n",
        "# Define a function to calculate fairness metrics for race/sex\n",
        "def calculate_fairness_metrics(mask, y_pred, y_test):\n",
        "    y_pred_group = y_pred[mask]\n",
        "    y_test_group = y_test[mask]\n",
        "    tn_group, fp_group, fn_group, tp_group = confusion_matrix(y_test_group, y_pred_group).ravel()\n",
        "    TPR_group = tp_group / (tp_group + fn_group)\n",
        "    FPR_group = fp_group / (fp_group + tn_group)\n",
        "    FNR_group = fn_group / (fn_group + tp_group)\n",
        "    return TPR_group, FPR_group, FNR_group\n",
        "\n",
        "# Separate by 'sex' (male vs female) and 'race' (white vs others)\n",
        "# Mask for sex groups (Male = 1, Female = 0)\n",
        "sex_mask_male = X_test['sex'] == 1\n",
        "sex_mask_female = X_test['sex'] == 0\n",
        "\n",
        "# Mask for race groups (White = 1, others = 0)\n",
        "race_mask_white = X_test['race'] == 1\n",
        "race_mask_others = X_test['race'] == 0\n",
        "\n",
        "# Calculate fairness metrics for each group\n",
        "# For sex (Male vs Female)\n",
        "TPR_male, FPR_male, FNR_male = calculate_fairness_metrics(sex_mask_male, y_pred, y_test)\n",
        "TPR_female, FPR_female, FNR_female = calculate_fairness_metrics(sex_mask_female, y_pred, y_test)\n",
        "\n",
        "# For race (White vs Others)\n",
        "TPR_white, FPR_white, FNR_white = calculate_fairness_metrics(race_mask_white, y_pred, y_test)\n",
        "TPR_others, FPR_others, FNR_others = calculate_fairness_metrics(race_mask_others, y_pred, y_test)\n",
        "\n",
        "# Print fairness metrics for sex\n",
        "print(f\"\\nFairness Metrics for Male Group:\")\n",
        "print(f\"True Positive Rate (TPR) - Male: {TPR_male:.4f}\")\n",
        "print(f\"False Positive Rate (FPR) - Male: {FPR_male:.4f}\")\n",
        "print(f\"False Negative Rate (FNR) - Male: {FNR_male:.4f}\")\n",
        "\n",
        "print(f\"\\nFairness Metrics for Female Group:\")\n",
        "print(f\"True Positive Rate (TPR) - Female: {TPR_female:.4f}\")\n",
        "print(f\"False Positive Rate (FPR) - Female: {FPR_female:.4f}\")\n",
        "print(f\"False Negative Rate (FNR) - Female: {FNR_female:.4f}\")\n",
        "\n",
        "# Print fairness metrics for race\n",
        "print(f\"\\nFairness Metrics for White Group:\")\n",
        "print(f\"True Positive Rate (TPR) - White: {TPR_white:.4f}\")\n",
        "print(f\"False Positive Rate (FPR) - White: {FPR_white:.4f}\")\n",
        "print(f\"False Negative Rate (FNR) - White: {FNR_white:.4f}\")\n",
        "\n",
        "print(f\"\\nFairness Metrics for Other Races Group:\")\n",
        "print(f\"True Positive Rate (TPR) - Others: {TPR_others:.4f}\")\n",
        "print(f\"False Positive Rate (FPR) - Others: {FPR_others:.4f}\")\n",
        "print(f\"False Negative Rate (FNR) - Others: {FNR_others:.4f}\")\n",
        "\n",
        "# Calculate Equality of Opportunity (Separation) for sex and race\n",
        "# For sex:\n",
        "equality_of_opportunity_sex = abs(TPR_male - TPR_female)\n",
        "\n",
        "# For race:\n",
        "equality_of_opportunity_race = abs(TPR_white - TPR_others)\n",
        "\n",
        "# Calculate Equality of Odds (FPR and FNR) for sex and race\n",
        "# For sex:\n",
        "equality_of_odds_fpr_sex = abs(FPR_male - FPR_female)\n",
        "equality_of_odds_fnr_sex = abs(FNR_male - FNR_female)\n",
        "\n",
        "# For race:\n",
        "equality_of_odds_fpr_race = abs(FPR_white - FPR_others)\n",
        "equality_of_odds_fnr_race = abs(FNR_white - FNR_others)\n",
        "\n",
        "# Print Equality of Opportunity and Equality of Odds results\n",
        "print(f\"\\nEquality of Opportunity (Separation) between Male and Female: {equality_of_opportunity_sex:.4f}\")\n",
        "print(f\"Equality of Opportunity (Separation) between White and Other Races: {equality_of_opportunity_race:.4f}\")\n",
        "\n",
        "print(f\"\\nEquality of Odds difference in FPR between Male and Female: {equality_of_odds_fpr_sex:.4f}\")\n",
        "print(f\"Equality of Odds difference in FNR between Male and Female: {equality_of_odds_fnr_sex:.4f}\")\n",
        "\n",
        "print(f\"\\nEquality of Odds difference in FPR between White and Others: {equality_of_odds_fpr_race:.4f}\")\n",
        "print(f\"Equality of Odds difference in FNR between White and Others: {equality_of_odds_fnr_race:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l773vPiRde_D",
        "outputId": "d6952525-5cf4-43a8-c8ab-342e505d5572"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall ROC AUC: 0.8999\n",
            "Overall TPR: 0.6154, FPR: 0.0732, FNR: 0.3846\n",
            "\n",
            "Fairness Metrics for Male Group:\n",
            "True Positive Rate (TPR) - Male: 0.6302\n",
            "False Positive Rate (FPR) - Male: 0.1020\n",
            "False Negative Rate (FNR) - Male: 0.3698\n",
            "\n",
            "Fairness Metrics for Female Group:\n",
            "True Positive Rate (TPR) - Female: 0.5294\n",
            "False Positive Rate (FPR) - Female: 0.0280\n",
            "False Negative Rate (FNR) - Female: 0.4706\n",
            "\n",
            "Fairness Metrics for White Group:\n",
            "True Positive Rate (TPR) - White: 0.6167\n",
            "False Positive Rate (FPR) - White: 0.0794\n",
            "False Negative Rate (FNR) - White: 0.3833\n",
            "\n",
            "Fairness Metrics for Other Races Group:\n",
            "True Positive Rate (TPR) - Others: 0.6020\n",
            "False Positive Rate (FPR) - Others: 0.0402\n",
            "False Negative Rate (FNR) - Others: 0.3980\n",
            "\n",
            "Equality of Opportunity (Separation) between Male and Female: 0.1008\n",
            "Equality of Opportunity (Separation) between White and Other Races: 0.0147\n",
            "\n",
            "Equality of Odds difference in FPR between Male and Female: 0.0741\n",
            "Equality of Odds difference in FNR between Male and Female: 0.1008\n",
            "\n",
            "Equality of Odds difference in FPR between White and Others: 0.0392\n",
            "Equality of Odds difference in FNR between White and Others: 0.0147\n"
          ]
        }
      ]
    }
  ]
}