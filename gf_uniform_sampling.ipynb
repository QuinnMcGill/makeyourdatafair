{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20feb1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "BEFORE UNIFORM SAMPLING (ORIGINAL DATA)\n",
      "========================================\n",
      "\n",
      "Dataset Fairness Metrics (Original):\n",
      "Statistical Parity Difference: -0.1963\n",
      "Disparate Impact: 0.3580\n",
      "\n",
      "Model Performance (Original):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90      4976\n",
      "           1       0.72      0.58      0.64      1537\n",
      "\n",
      "    accuracy                           0.85      6513\n",
      "   macro avg       0.80      0.76      0.77      6513\n",
      "weighted avg       0.84      0.85      0.84      6513\n",
      "\n",
      "\n",
      "Model Fairness Metrics (Original):\n",
      "Equal Opportunity Difference: -0.0777\n",
      "Average Odds Difference: -0.0781\n",
      "Disparate Impact (Predictions): 0.3051\n",
      "\n",
      "========================================\n",
      "AFTER UNIFORM SAMPLING\n",
      "========================================\n",
      "\n",
      "Dataset Fairness Metrics (After Sampling):\n",
      "Statistical Parity Difference: 0.0000\n",
      "Disparate Impact: 1.0000\n",
      "\n",
      "Model Performance (After Sampling):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81       474\n",
      "           1       0.80      0.84      0.82       470\n",
      "\n",
      "    accuracy                           0.82       944\n",
      "   macro avg       0.82      0.82      0.82       944\n",
      "weighted avg       0.82      0.82      0.82       944\n",
      "\n",
      "\n",
      "Model Fairness Metrics (After Sampling):\n",
      "Equal Opportunity Difference: 0.0176\n",
      "Average Odds Difference: -0.0489\n",
      "Disparate Impact (Predictions): 0.8524\n",
      "\n",
      "========================================\n",
      "IMPROVEMENT COMPARISON\n",
      "========================================\n",
      "\n",
      "Dataset Fairness Improvement:\n",
      "Statistical Parity Difference: +0.1963 (Before: -0.1963, After: 0.0000)\n",
      "Disparate Impact: +0.6420 (Before: 0.3580, After: 1.0000)\n",
      "\n",
      "Model Fairness Improvement:\n",
      "Equal Opportunity Difference: +0.0953 (Before: -0.0777, After: 0.0176)\n",
      "Average Odds Difference: +0.0292 (Before: -0.0781, After: -0.0489)\n",
      "Disparate Impact (Predictions): +0.5474 (Before: 0.3051, After: 0.8524)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# 1. Load and Prepare Data\n",
    "def load_data():\n",
    "    import kagglehub\n",
    "    path = kagglehub.dataset_download(\"uciml/adult-census-income\")\n",
    "    df = pd.read_csv(path + '/adult.csv')\n",
    "    df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "    \n",
    "    # Convert target and protected attribute\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})\n",
    "    df['sex'] = df['sex'].map({'Female': 0, 'Male': 1})\n",
    "    \n",
    "    # Convert other categorical columns to numerical codes\n",
    "    categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', \n",
    "                       'relationship', 'race', 'native-country']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "# 2. Uniform Sampling Preprocessing\n",
    "def uniform_sampling(df, target, protected_attr):\n",
    "    groups = [\n",
    "        (df[protected_attr] == 0) & (df[target] == 0),  # Female, low income\n",
    "        (df[protected_attr] == 0) & (df[target] == 1),  # Female, high income\n",
    "        (df[protected_attr] == 1) & (df[target] == 0),  # Male, low income\n",
    "        (df[protected_attr] == 1) & (df[target] == 1)   # Male, high income\n",
    "    ]\n",
    "    \n",
    "    min_size = min([sum(g) for g in groups])\n",
    "    sampled_dfs = [df[g].sample(min_size, random_state=42) for g in groups]\n",
    "    return pd.concat(sampled_dfs)\n",
    "\n",
    "# 3. Fairness Evaluation Utilities\n",
    "def evaluate_dataset_fairness(df, target, protected_attr):\n",
    "    \"\"\"Evaluate fairness at dataset level (before/after sampling)\"\"\"\n",
    "    dataset = BinaryLabelDataset(\n",
    "        df=df,\n",
    "        label_names=[target],\n",
    "        protected_attribute_names=[protected_attr]\n",
    "    )\n",
    "    metric = BinaryLabelDatasetMetric(\n",
    "        dataset,\n",
    "        unprivileged_groups=[{protected_attr: 0}],  # Female\n",
    "        privileged_groups=[{protected_attr: 1}]      # Male\n",
    "    )\n",
    "    return {\n",
    "        'Statistical Parity Difference': metric.statistical_parity_difference(),\n",
    "        'Disparate Impact': metric.disparate_impact()\n",
    "    }\n",
    "\n",
    "def evaluate_model_fairness(y_true, y_pred, protected_attr):\n",
    "    \"\"\"Evaluate fairness of model predictions\"\"\"\n",
    "    dataset_true = BinaryLabelDataset(\n",
    "        df=pd.DataFrame({'y_true': y_true, 'protected': protected_attr}),\n",
    "        label_names=['y_true'],\n",
    "        protected_attribute_names=['protected']\n",
    "    )\n",
    "    dataset_pred = dataset_true.copy()\n",
    "    dataset_pred.labels = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    metric = ClassificationMetric(\n",
    "        dataset_true,\n",
    "        dataset_pred,\n",
    "        unprivileged_groups=[{'protected': 0}],  # Female\n",
    "        privileged_groups=[{'protected': 1}]      # Male\n",
    "    )\n",
    "    return {\n",
    "        'Equal Opportunity Difference': metric.equal_opportunity_difference(),\n",
    "        'Average Odds Difference': metric.average_odds_difference(),\n",
    "        'Disparate Impact (Predictions)': BinaryLabelDatasetMetric(\n",
    "            dataset_pred,\n",
    "            unprivileged_groups=[{'protected': 0}],\n",
    "            privileged_groups=[{'protected': 1}]\n",
    "        ).disparate_impact()\n",
    "    }\n",
    "\n",
    "# 4. Main Pipeline\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_data()\n",
    "    \n",
    "    # =================================================================\n",
    "    # BEFORE SAMPLING (Original Data)\n",
    "    # =================================================================\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"BEFORE UNIFORM SAMPLING (ORIGINAL DATA)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Evaluate original dataset fairness\n",
    "    orig_fairness = evaluate_dataset_fairness(df, 'income', 'sex')\n",
    "    print(\"\\nDataset Fairness Metrics (Original):\")\n",
    "    for metric, value in orig_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Train model on original data\n",
    "    X_orig = df.drop(columns=['income', 'fnlwgt'])\n",
    "    y_orig = df['income']\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X_orig, y_orig, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), \n",
    "         ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country'])\n",
    "    ])\n",
    "    \n",
    "    model_orig = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    model_orig.fit(X_train_orig, y_train_orig)\n",
    "    \n",
    "    # Evaluate original model\n",
    "    y_pred_orig = model_orig.predict(X_test_orig)\n",
    "    print(\"\\nModel Performance (Original):\")\n",
    "    print(classification_report(y_test_orig, y_pred_orig))\n",
    "    \n",
    "    orig_model_fairness = evaluate_model_fairness(y_test_orig, y_pred_orig, X_test_orig['sex'])\n",
    "    print(\"\\nModel Fairness Metrics (Original):\")\n",
    "    for metric, value in orig_model_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # AFTER SAMPLING\n",
    "    # =================================================================\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"AFTER UNIFORM SAMPLING\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Apply uniform sampling\n",
    "    df_sampled = uniform_sampling(df, 'income', 'sex')\n",
    "    \n",
    "    # Evaluate sampled dataset fairness\n",
    "    sampled_fairness = evaluate_dataset_fairness(df_sampled, 'income', 'sex')\n",
    "    print(\"\\nDataset Fairness Metrics (After Sampling):\")\n",
    "    for metric, value in sampled_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Train model on sampled data\n",
    "    X_sampled = df_sampled.drop(columns=['income', 'fnlwgt'])\n",
    "    y_sampled = df_sampled['income']\n",
    "    X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = train_test_split(\n",
    "        X_sampled, y_sampled, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    model_sampled = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    model_sampled.fit(X_train_sampled, y_train_sampled)\n",
    "    \n",
    "    # Evaluate sampled model\n",
    "    y_pred_sampled = model_sampled.predict(X_test_sampled)\n",
    "    print(\"\\nModel Performance (After Sampling):\")\n",
    "    print(classification_report(y_test_sampled, y_pred_sampled))\n",
    "    \n",
    "    sampled_model_fairness = evaluate_model_fairness(y_test_sampled, y_pred_sampled, X_test_sampled['sex'])\n",
    "    print(\"\\nModel Fairness Metrics (After Sampling):\")\n",
    "    for metric, value in sampled_model_fairness.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # IMPROVEMENT COMPARISON\n",
    "    # =================================================================\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"IMPROVEMENT COMPARISON\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(\"\\nDataset Fairness Improvement:\")\n",
    "    for metric in orig_fairness:\n",
    "        improvement = sampled_fairness[metric] - orig_fairness[metric]\n",
    "        print(f\"{metric}: {improvement:+.4f} (Before: {orig_fairness[metric]:.4f}, After: {sampled_fairness[metric]:.4f})\")\n",
    "    \n",
    "    print(\"\\nModel Fairness Improvement:\")\n",
    "    for metric in orig_model_fairness:\n",
    "        improvement = sampled_model_fairness[metric] - orig_model_fairness[metric]\n",
    "        print(f\"{metric}: {improvement:+.4f} (Before: {orig_model_fairness[metric]:.4f}, After: {sampled_model_fairness[metric]:.4f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
