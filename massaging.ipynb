{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model for Census Income Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, accuracy_score, classification_report, recall_score, f1_score\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = os.path.join(\"..\", \"data\", \"income\", \"adult.data\") # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define column names\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', \n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "                'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# Assign column names to the DataFrame\n",
    "df.columns = column_names\n",
    "\n",
    "# Divide the features into numerical and non-numerical lists\n",
    "# Extract numerical and string features\n",
    "num_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_features = df.select_dtypes(include=['object', 'string']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "1   38            Private  215646     HS-grad              9   \n",
      "2   53            Private  234721        11th              7   \n",
      "3   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race  sex  \\\n",
      "0   Married-civ-spouse     Exec-managerial         Husband   White    1   \n",
      "1             Divorced   Handlers-cleaners   Not-in-family   White    1   \n",
      "2   Married-civ-spouse   Handlers-cleaners         Husband   Black    1   \n",
      "3   Married-civ-spouse      Prof-specialty            Wife   Black    0   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week  native-country  income  \n",
      "0             0             0              13   United-States       0  \n",
      "1             0             0              40   United-States       0  \n",
      "2             0             0              40   United-States       0  \n",
      "3             0             0              40            Cuba       0  \n"
     ]
    }
   ],
   "source": [
    "# Convert income column to binary, flag errors\n",
    "def convert_income(value):\n",
    "    value = str(value).strip()\n",
    "    if value == '>50K':\n",
    "        return 1\n",
    "    elif value == '<=50K':\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan  # Flag invalid values as NaN (or set a custom error flag)\n",
    "\n",
    "df['income'] = df['income'].apply(convert_income)\n",
    "\n",
    "# Encode the privileged/underprivileged\n",
    "def encode_sex(value):\n",
    "    value = str(value).strip()\n",
    "    if value == \"Male\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  \n",
    "\n",
    "df['sex'] = df['sex'].apply(encode_sex)\n",
    "\n",
    "# Identify and display rows with errors\n",
    "error_rows = df[df['income'].isna()]\n",
    "if not error_rows.empty:\n",
    "    print(\"Invalid income values found in \", error_rows.size, \"rows: \")\n",
    "    print(error_rows)\n",
    "\n",
    "\n",
    "print(df.head(4))\n",
    "cat_features.remove('income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_performance(y_test, y_pred):\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Display metrics\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features and target variable\n",
    "# X = df[num_features + cat_features]\n",
    "# y = df['income']\n",
    "\n",
    "# # Preprocessing: Standardize numerical features and one-hot encode categorical features\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     #('num', StandardScaler(), num_features),\n",
    "#     ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "# ])\n",
    "\n",
    "# # Create a pipeline with logistic regression\n",
    "# model = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', LogisticRegression(max_iter=1000))\n",
    "# ])\n",
    "\n",
    "# # Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate performance of baseline model\n",
    "# eval_performance(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Characterizing Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Fairness: \n",
    "\n",
    "Ensures that similar individuals receive similar predictions. A common way to assess this is by checking prediction consistency across similar instances. We can do this using a Nearest Neighbors Consistency Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gower\n",
    "\n",
    "# ---  HEADS UP!! This worked, but took 8 mins last time ---\n",
    "# I created a flag to stop it from always running.\n",
    "# --- RESULTS ----\n",
    "# Baseline model: 0.8276 with k = 5\n",
    "\n",
    "ind_fairness_flag = False\n",
    "\n",
    "if ind_fairness_flag == True:\n",
    "    # Scale numerical features & One-Hot Encode categorical features\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)\n",
    "    ])\n",
    "\n",
    "    # Transform training and test features\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Compute Gower distance matrix for test samples w.r.t training data\n",
    "    gower_distances = gower.gower_matrix(X_test, X_train)  # Shape: (num_test_samples, num_train_samples)\n",
    "\n",
    "    # Find k nearest neighbors (excluding self)\n",
    "    k = 5  # Adjust as needed\n",
    "    neighbors = np.argsort(gower_distances, axis=1)[:, 1:k+1]  # Get indices of k nearest neighbors\n",
    "\n",
    "    # Get model predictions\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute consistency score: Fraction of nearest neighbors with same prediction\n",
    "    consistencies = []\n",
    "    for i, neigh_indices in enumerate(neighbors):\n",
    "        neighbor_preds = y_train.iloc[neigh_indices]  # Get predictions of k neighbors from training labels\n",
    "        consistency = np.mean(neighbor_preds == y_test_pred[i])  # Fraction with same prediction\n",
    "        consistencies.append(consistency)\n",
    "\n",
    "    # Calculate overall consistency score\n",
    "    individual_fairness_score = np.mean(consistencies)\n",
    "\n",
    "    print(f'Individual Fairness Consistency Score (with categorical features): {individual_fairness_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masaging to Remove Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def compute_discrimination(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Compute the discrimination score (difference in positive outcome rates).\n",
    "    privileged = df[df[sensitive_attr] == privileged_value]\n",
    "    unprivileged = df[df[sensitive_attr] != privileged_value]\n",
    "\n",
    "    pos_rate_privileged = sum(privileged[class_attr] == positive_class) / len(privileged)\n",
    "    pos_rate_unprivileged = sum(unprivileged[class_attr] == positive_class) / len(unprivileged)\n",
    "\n",
    "    return pos_rate_unprivileged - pos_rate_privileged\n",
    "\n",
    "def compute_m(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Compute number of instances M to relabel.\n",
    "    disc = compute_discrimination(df, sensitive_attr, class_attr, privileged_value, positive_class)\n",
    "    \n",
    "    n_privileged = len(df[df[sensitive_attr] == privileged_value])\n",
    "    n_unprivileged = len(df[df[sensitive_attr] != privileged_value])\n",
    "    \n",
    "    return int(abs(disc) * (n_privileged * n_unprivileged) / len(df))\n",
    "\n",
    "def rank_instances(df, features, sensitive_attr, class_attr):\n",
    "    # Train a classifier to rank instances by likelihood of being positive.\n",
    "    X = df[features]\n",
    "    y = df[class_attr]\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    scores = model.predict_proba(X)[:, 1]  # Probability of positive class\n",
    "    df['score'] = scores\n",
    "    return df\n",
    "\n",
    "def apply_massaging(df, sensitive_attr, class_attr, privileged_value, positive_class):\n",
    "    # Perform massaging technique.\n",
    "    # Step 1: Compute M\n",
    "    M = compute_m(df, sensitive_attr, class_attr, privileged_value, positive_class)\n",
    "    print(f\"Number of label changes (M): {M}\")\n",
    "\n",
    "    if M == 0:\n",
    "        print(\"No massaging needed.\")\n",
    "        return df\n",
    "\n",
    "    # Step 2: Rank instances\n",
    "    features = [col for col in df.columns if col not in [sensitive_attr, class_attr]]\n",
    "    df = rank_instances(df, features, sensitive_attr, class_attr)\n",
    "\n",
    "    # Step 3: Modify labels\n",
    "    unprivileged_neg = df[(df[sensitive_attr] != privileged_value) & (df[class_attr] != positive_class)]\n",
    "    privileged_pos = df[(df[sensitive_attr] == privileged_value) & (df[class_attr] == positive_class)]\n",
    "\n",
    "    # Promote top M from unprivileged_neg\n",
    "    df.loc[unprivileged_neg.nlargest(M, 'score').index, class_attr] = positive_class\n",
    "\n",
    "    # Demote bottom M from privileged_pos\n",
    "    df.loc[privileged_pos.nsmallest(M, 'score').index, class_attr] = 1 - positive_class\n",
    "\n",
    "    # Drop the ranking column\n",
    "    df.drop(columns=['score'], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "       age          workclass  fnlwgt    education  education-num  \\\n",
      "0       50   Self-emp-not-inc   83311    Bachelors             13   \n",
      "1       38            Private  215646      HS-grad              9   \n",
      "2       53            Private  234721         11th              7   \n",
      "3       28            Private  338409    Bachelors             13   \n",
      "4       37            Private  284582      Masters             14   \n",
      "...    ...                ...     ...          ...            ...   \n",
      "32555   27            Private  257302   Assoc-acdm             12   \n",
      "32556   40            Private  154374      HS-grad              9   \n",
      "32557   58            Private  151910      HS-grad              9   \n",
      "32558   22            Private  201490      HS-grad              9   \n",
      "32559   52       Self-emp-inc  287927      HS-grad              9   \n",
      "\n",
      "            marital-status          occupation    relationship    race  sex  \\\n",
      "0       Married-civ-spouse     Exec-managerial         Husband   White    1   \n",
      "1                 Divorced   Handlers-cleaners   Not-in-family   White    1   \n",
      "2       Married-civ-spouse   Handlers-cleaners         Husband   Black    1   \n",
      "3       Married-civ-spouse      Prof-specialty            Wife   Black    0   \n",
      "4       Married-civ-spouse     Exec-managerial            Wife   White    0   \n",
      "...                    ...                 ...             ...     ...  ...   \n",
      "32555   Married-civ-spouse        Tech-support            Wife   White    0   \n",
      "32556   Married-civ-spouse   Machine-op-inspct         Husband   White    1   \n",
      "32557              Widowed        Adm-clerical       Unmarried   White    0   \n",
      "32558        Never-married        Adm-clerical       Own-child   White    1   \n",
      "32559   Married-civ-spouse     Exec-managerial            Wife   White    0   \n",
      "\n",
      "       capital-gain  capital-loss  hours-per-week  native-country  income  \n",
      "0                 0             0              13   United-States       0  \n",
      "1                 0             0              40   United-States       0  \n",
      "2                 0             0              40   United-States       0  \n",
      "3                 0             0              40            Cuba       0  \n",
      "4                 0             0              40   United-States       0  \n",
      "...             ...           ...             ...             ...     ...  \n",
      "32555             0             0              38   United-States       0  \n",
      "32556             0             0              40   United-States       1  \n",
      "32557             0             0              40   United-States       0  \n",
      "32558             0             0              20   United-States       0  \n",
      "32559         15024             0              40   United-States       1  \n",
      "\n",
      "[32560 rows x 15 columns]\n",
      "Number of label changes (M): 1414\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' Self-emp-not-inc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17816\\501536169.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m print(\u001b[33m\"Before:\"\u001b[39m)\n\u001b[32m      2\u001b[39m print(df)\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Apply massaging technique\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_massaged = apply_massaging(df, \u001b[33m'sex'\u001b[39m, \u001b[33m'income'\u001b[39m, privileged_value=\u001b[32m1\u001b[39m, positive_class=\u001b[32m1\u001b[39m)\n\u001b[32m      6\u001b[39m print(\u001b[33m\"After:\"\u001b[39m)\n\u001b[32m      7\u001b[39m print(df_massaged)\n\u001b[32m      8\u001b[39m \n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17816\\4194104644.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, sensitive_attr, class_attr, privileged_value, positive_class)\u001b[39m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Step 2: Rank instances\u001b[39;00m\n\u001b[32m     46\u001b[39m     features = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;28;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m [sensitive_attr, class_attr]]\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     df = rank_instances(df, features, sensitive_attr, class_attr)\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Step 3: Modify labels\u001b[39;00m\n\u001b[32m     50\u001b[39m     unprivileged_neg = df[(df[sensitive_attr] != privileged_value) & (df[class_attr] != positive_class)]\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17816\\4194104644.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, features, sensitive_attr, class_attr)\u001b[39m\n\u001b[32m     25\u001b[39m     X = df[features]\n\u001b[32m     26\u001b[39m     y = df[class_attr]\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m     model = DecisionTreeClassifier()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     model.fit(X, y)\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m     scores = model.predict_proba(X)[:, \u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Probability of positive class\u001b[39;00m\n\u001b[32m     32\u001b[39m     df[\u001b[33m'score'\u001b[39m] = scores\n",
      "\u001b[32mc:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1385\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m                 )\n\u001b[32m   1388\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32mc:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1020\u001b[39m         self : DecisionTreeClassifier\n\u001b[32m   1021\u001b[39m             Fitted estimator.\n\u001b[32m   1022\u001b[39m         \"\"\"\n\u001b[32m   1023\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m         super()._fit(\n\u001b[32m   1025\u001b[39m             X,\n\u001b[32m   1026\u001b[39m             y,\n\u001b[32m   1027\u001b[39m             sample_weight=sample_weight,\n",
      "\u001b[32mc:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    248\u001b[39m             check_X_params = dict(\n\u001b[32m    249\u001b[39m                 dtype=DTYPE, accept_sparse=\u001b[33m\"csc\"\u001b[39m, ensure_all_finite=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    250\u001b[39m             )\n\u001b[32m    251\u001b[39m             check_y_params = dict(ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m             X, y = validate_data(\n\u001b[32m    253\u001b[39m                 self, X, y, validate_separately=(check_X_params, check_y_params)\n\u001b[32m    254\u001b[39m             )\n\u001b[32m    255\u001b[39m \n",
      "\u001b[32mc:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m             \u001b[38;5;66;03m# :(\u001b[39;00m\n\u001b[32m   2953\u001b[39m             check_X_params, check_y_params = validate_separately\n\u001b[32m   2954\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"estimator\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m check_X_params:\n\u001b[32m   2955\u001b[39m                 check_X_params = {**default_check_params, **check_X_params}\n\u001b[32m-> \u001b[39m\u001b[32m2956\u001b[39m             X = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_X_params)\n\u001b[32m   2957\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"estimator\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m check_y_params:\n\u001b[32m   2958\u001b[39m                 check_y_params = {**default_check_params, **check_y_params}\n\u001b[32m   2959\u001b[39m             y = check_array(y, input_name=\u001b[33m\"y\"\u001b[39m, **check_y_params)\n",
      "\u001b[32mc:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1052\u001b[39m                         )\n\u001b[32m   1053\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1055\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m                 raise ValueError(\n\u001b[32m   1058\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32mc:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    835\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    836\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    837\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    840\u001b[39m \n\u001b[32m    841\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2149\u001b[39m     def __array__(\n\u001b[32m   2150\u001b[39m         self, dtype: npt.DTypeLike | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, copy: bool_t | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2151\u001b[39m     ) -> np.ndarray:\n\u001b[32m   2152\u001b[39m         values = self._values\n\u001b[32m-> \u001b[39m\u001b[32m2153\u001b[39m         arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2154\u001b[39m         if (\n\u001b[32m   2155\u001b[39m             astype_is_view(values.dtype, arr.dtype)\n\u001b[32m   2156\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m using_copy_on_write()\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: ' Self-emp-not-inc'"
     ]
    }
   ],
   "source": [
    "print(\"Before:\")\n",
    "print(df)\n",
    "\n",
    "# Apply massaging technique\n",
    "df_massaged = apply_massaging(df, 'sex', 'income', privileged_value=1, positive_class=1)\n",
    "print(\"After:\")\n",
    "print(df_massaged)\n",
    "\n",
    "X = df[num_features + cat_features]\n",
    "y = df['income']\n",
    "\n",
    "# Preprocessing: Standardize numerical features and one-hot encode categorical features\n",
    "preprocessor = ColumnTransformer([\n",
    "    #('num', StandardScaler(), num_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "])\n",
    "\n",
    "# Create a pipeline with logistic regression\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance of baseline model\n",
    "eval_performance(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
